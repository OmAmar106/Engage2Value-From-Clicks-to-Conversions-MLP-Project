{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99546,"databundleVersionId":11895149,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T22:35:48.009598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n\nX = df.drop(\"purchaseValue\",axis=1)\nY = df[\"purchaseValue\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=59)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_predict = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(pd.concat([X_train,Y_train],axis=1).select_dtypes(include=['number']).corr(), annot=True, cmap='coolwarm', fmt=\".2f\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Lkeep = ['device.mobileDeviceBranding', 'gclIdPresent',\n# #        'totals.visits','trafficSource.adwordsClickInfo.isVideoAd','pageViews',\n# #        'device.mobileDeviceModel','totals.bounces','totalHits','device.isMobile','new_visits']\n# Lkeep = list(df.columns)\n# L7 = ['userId','sessionId','trafficSource.adwordsClickInfo.page','sessionStart','geoNetwork.region',\n#       'trafficSource.adwordsClickInfo.adNetworkType','geoNetwork.city','geoNetwork.metro','trafficSource.referralPath'] \n# # They had very distributed values, or had too many not in dataset values, so removed them, checked each of them by doing value_counts()\n\n# for j in L7:\n#     Lkeep.remove(j)\n# # ['purchaseValue', 'browser', 'trafficSource.adContent', 'trafficSource.keyword', 'geoCluster', 'trafficSource.adwordsClickInfo.slot', 'userId', 'trafficSource.campaign', 'geoNetwork.networkDomain', 'gclIdPresent', 'sessionNumber', 'geoNetwork.region', 'trafficSource', 'sessionId', 'os', 'geoNetwork.subContinent', 'trafficSource.medium', 'locationCountry', 'trafficSource.adwordsClickInfo.adNetworkType', 'geoNetwork.city', 'trafficSource.adwordsClickInfo.page', 'geoNetwork.metro', 'pageViews', 'trafficSource.referralPath', 'date', 'deviceType', 'userChannel', 'totalHits', 'sessionStart', 'geoNetwork.continent', 'device.isMobile']\n\n# for i in df.columns:\n#     if len(df[i].value_counts())==1:\n#         if i in Lkeep:\n#             print(i)\n#             Lkeep.remove(i)\n# Lkeep.remove('purchaseValue')\n# # print(Lkeep)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train = X_train[Lkeep]\n# X_test = X_test[Lkeep]\n# X_predict = X_predict[Lkeep]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.heatmap(pd.concat([X_train,Y_train],axis=1).select_dtypes(include=['number']).corr(), annot=True, cmap='coolwarm', fmt=\".2f\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(X_train.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##################\n\n\nbrowser -> One hot encoding maybe? but too many browsers are there, but it does matter quite a bit\nmaybe i could just do for the popular browsers\n\ntrafficSource.adContent -> might be null and string based\n\ntrafficSource.keyword -> might be null and string based\n\ngeoCluster -> divides into 5 regions and is not null\n\ntrafficSource.adwordsClickInfo.slot -> 3 different possible values\n\ntrafficSource.campaign -> String based and not null\n\ngeoNetwork.networkDomain -> 3 possible domains, not null, do one hot encoding\n\nsessionNumber -> not null , has a unexpected correlation with purchaseValue\n\ntrafficSource -> not null, string based, too many values\n\nos -> around 20 not null, could help\n\ngeoNetwork.subContinent -> zone wise continent, not null, string, will help\n\ntrafficSource.medium -> medium for reaching, not null, string, around 7 distinct values\n\nlocationCountry -> String not null, too many distinct but will help for sure\n\npageViews -> Integer not null\n\ndeviceType -> oneHotEncoding, only 3 values present not null\n\nuserChannel -> not null 8 distinct values\n\ntotalHits -> not null Integer\n\ngeoNetwork.continent -> Continent Not Null\n\ndevice.isMobile -> True of False not null","metadata":{}},{"cell_type":"code","source":"print(df['device.isMobile'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# import pandas as pd\n\n# def preprocess_df(df, top_n=10, scaler=None, fit_scaler=True, col_order=None, num_cols_ref=None):\n#     df = df.copy()\n\n#     str_cols = df.select_dtypes(include='object').columns\n#     df[str_cols] = df[str_cols].fillna(\"Missing\")\n#     df['device.isMobile'] = df['device.isMobile'].astype(int)\n\n#     for col in ['trafficSource', 'trafficSource.campaign']:\n#         freq = df[col].value_counts().to_dict()\n#         df[col + \"_freq\"] = df[col].map(freq)\n\n#     for col in ['browser', 'os']:\n#         top = df[col].value_counts().nlargest(top_n).index\n#         for cat in top:\n#             df[f\"{col}_{cat}\"] = (df[col] == cat).astype(int)\n#         df[f\"{col}_Other\"] = (~df[col].isin(top)).astype(int)\n#         df.drop(columns=col, inplace=True)\n\n#     low_card = [\n#         'geoNetwork.networkDomain',\n#         'trafficSource.adwordsClickInfo.slot',\n#         'deviceType',\n#         'userChannel',\n#         'trafficSource.medium',\n#         'geoNetwork.subContinent',\n#         'geoNetwork.continent',\n#         'geoCluster'\n#     ]\n#     df = pd.get_dummies(df, columns=low_card, prefix=low_card)\n\n#     if col_order is not None:\n#         df = df.reindex(columns=col_order, fill_value=0)\n\n#     if num_cols_ref is None:\n#         num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n#     else:\n#         num_cols = num_cols_ref\n\n#     if scaler is None and fit_scaler:\n#         scaler = StandardScaler()\n#         df[num_cols] = scaler.fit_transform(df[num_cols])\n#     elif scaler is not None:\n#         df[num_cols] = scaler.transform(df[num_cols])\n\n#     if col_order is None:\n#         col_order = df.columns.tolist()\n#     if num_cols_ref is None:\n#         num_cols_ref = num_cols.tolist()\n\n#     return df, scaler, col_order, num_cols_ref\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [X_train, X_test, X_predict]:\n#     df['date'] = pd.to_datetime(df['date'], errors='coerce')\n# for df in [X_train, X_test, X_predict]:\n#     df['year'] = df['date'].dt.year\n#     df['month'] = df['date'].dt.month\n#     df['day'] = df['date'].dt.day\n#     df['dayofweek'] = df['date'].dt.dayofweek\n#     df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n#     df['quarter'] = df['date'].dt.quarter\n#     df['dayofyear'] = df['date'].dt.dayofyear\n# for df in [X_train, X_test, X_predict]:\n#     df.drop('date', axis=1, inplace=True)\n# for df in [X_train, X_test, X_predict]:\n#     df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n#     df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n#     df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n#     df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import FunctionTransformer\n# import numpy as np\n# import pandas as pd\n\n# def replace_true_false(X):\n#     return np.where(X == True, 1, np.where(X == False, -1, X))\n\n# categorical_cols = ['geoNetwork.subContinent', 'geoCluster']\n\n# for df in [X_train, X_test, X_predict]:\n#     df.drop('sessionNumber', inplace=True, axis=1) # ise comment out krke bhi try krna, sometimes its better\n\n# numeric_cols = [col for col in X_train.columns if col not in categorical_cols]\n\n# preprocess = ColumnTransformer(\n#     transformers=[\n#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n#     ],\n#     remainder='passthrough'\n# )\n\n# X_train_enc = preprocess.fit_transform(X_train)\n# X_test_enc = preprocess.transform(X_test)\n# X_predict_enc = preprocess.transform(X_predict)\n\n# encoded_cat_cols = preprocess.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n# all_columns = list(encoded_cat_cols) + numeric_cols\n\n# X_train_enc = pd.DataFrame(X_train_enc, columns=all_columns, index=X_train.index)\n# X_test_enc = pd.DataFrame(X_test_enc, columns=all_columns, index=X_test.index)\n# X_predict_enc = pd.DataFrame(X_predict_enc, columns=all_columns, index=X_predict.index)\n\n# pipeline = Pipeline(steps=[('replace_true_false', FunctionTransformer(replace_true_false, validate=False))])\n\n# X_train_pre = pipeline.transform(X_train_enc)\n# X_test_pre = pipeline.transform(X_test_enc)\n# X_predict_pre = pipeline.transform(X_predict_enc)\n\n# X_train_pre = pd.DataFrame(X_train_pre, columns=all_columns, index=X_train.index)\n# X_test_pre = pd.DataFrame(X_test_pre, columns=all_columns, index=X_test.index)\n# X_predict_pre = pd.DataFrame(X_predict_pre, columns=all_columns, index=X_predict.index)\n\n# for col in X_train_pre.columns:\n#     try:\n#         X_train_pre[col] = X_train_pre[col].astype(int)\n#         X_test_pre[col] = X_test_pre[col].astype(int)\n#         X_predict_pre[col] = X_predict_pre[col].astype(int)\n#         # X_test_pre[col] = pd.to_numeric(X_test_pre[col], errors='coerce')\n#         # X_predict_pre[col] = pd.to_numeric(X_predict_pre[col], errors='coerce')\n#     except:\n#         pass\n        \n# X_train_pre = X_train_pre.select_dtypes(include=['number'])\n# X_test_pre = X_test_pre.select_dtypes(include=['number'])\n# X_predict_pre = X_predict_pre.select_dtypes(include=['number'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train_pre.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train_pre.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train_pre.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer,KNNImputer\n# from sklearn.preprocessing import OneHotEncoder,FunctionTransformer\n# from sklearn.compose import ColumnTransformer\n\n# def replace_true_false(X):\n#     return np.where(X == True, 1, np.where(X == False, 0, X))\n\n# pipeline = Pipeline(steps=[\n#     ('replace_true_false', FunctionTransformer(replace_true_false, validate=False)),\n#     ('imputer', KNNImputer(n_neighbors=5))\n# ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pipeline.fit(X_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train = pipeline.transform(X_train)\n# X_test = pipeline.transform(X_test)\n# X_predict = pipeline.transform(X_predict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n# # model = LinearRegression()\n# # rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)\n# # rf_model.fit(X_train,Y_train)\n# # from sklearn.linear_model import LinearRegression\n# # from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n# # from sklearn.ensemble import RandomForestRegressor\n\n# # rf_model = RandomForestRegressor()\n\n# # param_grid = {\n# #     'n_estimators': [100, 200, 300],\n# #     'max_depth': [5, 10, 20, None],\n# #     'min_samples_split': [2, 10, 20],\n# #     'min_samples_leaf': [1, 2, 4]\n# # }\n\n# # random_search = RandomizedSearchCV(\n# #     estimator=rf_model,\n# #     param_distributions=param_grid,\n# #     n_iter=10,\n# #     cv=2,\n# #     scoring='r2',\n# #     n_jobs=-1,\n# #     verbose=1\n# # )\n\n# # random_search.fit(X_train, Y_train)\n# # rf_model = random_search.best_estimator_\n\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import RidgeCV\n# from sklearn.preprocessing import PolynomialFeatures\n\n# model = Pipeline([\n#     ('scaler', StandardScaler()),\n#     ('poly',PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n#     ('ridge', RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100.0], cv=5))\n# ])\n# model.fit(X_train, Y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.pipeline import Pipeline\n# from sklearn.compose import ColumnTransformer\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.preprocessing import FunctionTransformer, StandardScaler\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.impute import SimpleImputer,KNNImputer\n# from sklearn.linear_model import Ridge,LinearRegression,Lasso\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.ensemble import HistGradientBoostingRegressor\n# from lightgbm import LGBMRegressor\n# import xgboost as xgb\n# from xgboost import XGBRegressor\n\n# # text_cols = [\n# #     'trafficSource.adContent',\n# #     'trafficSource.keyword',\n# #     'trafficSource.campaign',\n# #     'trafficSource',\n# #     'locationCountry'\n# # ]\n# # num_cols = [c for c in X_train_pre.columns if c not in text_cols]\n\n# # flatten = FunctionTransformer(lambda X: X.ravel(), validate=False)\n\n# # preprocessor = ColumnTransformer([\n# #     *[\n# #         (\n# #             f\"tfidf_{col}\",\n# #             Pipeline([\n# #                 ('impute', SimpleImputer(strategy='constant', fill_value='')),\n# #                 ('flatten', flatten),\n# #                 ('tfidf', TfidfVectorizer(max_features=100, stop_words='english'))\n# #             ]),\n# #             [col]\n# #         )\n# #         for col in text_cols\n# #     ],\n# #     (\n# #         'num',\n# #         Pipeline([\n# #             ('impute', SimpleImputer(strategy='mean')),\n# #             ('scale',   StandardScaler())\n# #         ]),\n# #         num_cols\n# #     )\n# # ], remainder='drop')\n# from sklearn.linear_model import RidgeCV\n\n# pipeline = Pipeline([\n#     # ('preproc', preprocessor),\n#     # ('to_dense', FunctionTransformer(lambda X: X.toarray(), accept_sparse=True)),\n#     ('impute',SimpleImputer(strategy='mean')),\n#     # ('imputer', KNNImputer(n_neighbors=5)),\n#     ('scale',StandardScaler()),\n#     # ('model', Ridge())\n#     ('poly',PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n#     # ('model', Lasso(alpha=0.1, max_iter=1000))\n#     # ('model',RandomForestRegressor(n_estimators=100,random_state=200,n_jobs=-1)),\n#     # ('reg', HistGradientBoostingRegressor(\n#     #     max_iter=200,\n#     #     learning_rate=0.05,\n#     #     max_depth=2,\n#     #     min_samples_leaf=10,\n#     #     l2_regularization=2.0,\n#     #     # random_state=22,\n#     #     # verbose=1\n#     # ))\n#     ('ridge', RidgeCV(alphas=[0.01], cv=10))\n#     # ('model', XGBRegressor(objective='reg:squarederror', max_depth=4, learning_rate=0.1, n_estimators=100, eval_metric='rmse', use_label_encoder=False))\n#     # ('lgbm', LGBMRegressor(\n#     #     n_estimators=1000,\n#     #     learning_rate=0.05,\n#     #     max_depth=6,\n#     #     random_state=42,\n#     #     n_jobs=-1\n#     # ))\n\n# ])\n\n\n# pipeline.fit(X_train_pre, Y_train)\n\n# from sklearn.metrics import r2_score\n\n# print(r2_score(Y_test,pipeline.predict(X_test_pre)))\n# print(r2_score(Y_train,pipeline.predict(X_train_pre)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n# from sklearn.impute import SimpleImputer\n# from sklearn.linear_model import RidgeCV, Ridge\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.metrics import r2_score\n\n# pipeline = Pipeline([\n#     ('impute', SimpleImputer(strategy='mean')),\n#     ('scale', StandardScaler()),\n#     ('poly', PolynomialFeatures(interaction_only=True,include_bias=False)),\n#     ('ridge', Ridge())\n# ])\n\n# param_grid = {\n#     'poly__degree': [2],\n#     'ridge__alpha': [0.01, 0.1, 1.0, 10, 100]\n# }\n\n# grid = GridSearchCV(\n#     pipeline,\n#     param_grid,\n#     scoring='r2',\n#     cv=5,\n#     n_jobs=-1,\n#     verbose=1\n# )\n\n# grid.fit(X_train_pre, Y_train)\n# pipeline = grid\n# print(\"Best Params:\", grid.best_params_)\n# print(\"Best R2 on Training Set:\", grid.best_score_)\n# print(\"Test R2:\", r2_score(Y_test, grid.predict(X_test_pre)))\n# print(\"Train R2:\", r2_score(Y_train, grid.predict(X_train_pre)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# xgb_model = XGBRegressor(\n#     objective='reg:squarederror',\n#     max_depth=3,\n#     learning_rate=0.05,\n#     n_estimators=500,\n#     eval_metric='rmse',\n#     use_label_encoder=False\n# )\n\n# xgb_model.fit(\n#     X_train_pre, Y_train,\n#     eval_set=[(X_test_pre, Y_test)],\n#     early_stopping_rounds=20,\n#     verbose=False\n# )\n\n# print(\"Train R2:\", r2_score(Y_train, xgb_model.predict(X_train_pre)))\n# print(\"Test R2:\", r2_score(Y_test, xgb_model.predict(X_test_pre)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndef preprocess(df, median_pageViews=None, ohe_model=None, categorical_cols_to_encode=None, fit_encoder=False,tfidf=None):\n    df = df.copy()\n\n    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df = df.drop('date', axis=1)\n\n    df['totals.bounces'] = df['totals.bounces'].fillna(0)\n    df['new_visits'] = df['new_visits'].fillna(0)\n\n    if median_pageViews is not None:\n        df['pageViews'] = df['pageViews'].fillna(median_pageViews)\n    else:\n        df['pageViews'] = df['pageViews'].fillna(df['pageViews'].median())\n\n    df['trafficSource.isTrueDirect'] = df['trafficSource.isTrueDirect'].astype(str).replace(\n        {'True': True, 'False': False, 'nan': False}\n    ).astype(bool)\n\n    categorical_fill_cols = {\n        'trafficSource.keyword': 'unknown', 'trafficSource.adContent': 'unknown_ad_content',\n        'trafficSource.adwordsClickInfo.adNetworkType': 'unknown_ad_network',\n        'trafficSource.adwordsClickInfo.slot': 'unknown_slot',\n        'trafficSource.referralPath': 'unknown_referral_path',\n        'trafficSource': 'unknown_traffic_source', 'trafficSource.campaign': 'unknown_campaign'\n    }\n\n    df['trafficSource.adwordsClickInfo.isVideoAd'] = df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True)\n    \n    df['trafficSource.adwordsClickInfo.isVideoAd'] = (\n        df['trafficSource.adwordsClickInfo.isVideoAd']\n        .astype(str)\n        .replace({'True': True, 'False': False, 'nan': True})\n        .astype(bool)\n        .astype(int)\n    )\n    \n    for col, fill_value in categorical_fill_cols.items():\n        if col in df.columns:\n            df[col] = df[col].fillna(fill_value)\n\n    Lvec = ['trafficSource.keyword','trafficSource.referralPath','trafficSource.adContent','trafficSource.campaign','trafficSource.adwordsClickInfo.adNetworkType']\n\n    for i in Lvec:\n        df[i] = df[i].fillna('(not provided)')\n        df[i] = df[i].replace('/',' ',regex=False)\n        df[i] = df[i].str.lower().str.strip()\n\n    \n    from sklearn.feature_extraction.text import TfidfVectorizer\n    if not tfidf:\n        tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n        tfidf.fit(df[Lvec])\n    keyword_tfidf = tfidf.transform(df[Lvec])\n    keyword_tfidf_df = pd.DataFrame(keyword_tfidf.toarray(), columns=tfidf.get_feature_names_out(), index=df.index)\n    df = pd.concat([df, keyword_tfidf_df], axis=1)\n    \n    df.drop(columns=Lvec, inplace=True)\n    \n    cols_to_drop = [\n        'device.browserSize', 'device.browserVersion', 'device.flashVersion', 'device.language',\n        'device.mobileDeviceBranding', 'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',\n        'device.mobileInputSelector', 'device.operatingSystemVersion', 'device.screenColors',\n        'device.screenResolution', 'geoNetwork.city', 'geoNetwork.metro', 'geoNetwork.networkDomain',\n        'geoNetwork.networkLocation', 'geoNetwork.region', 'socialEngagementType',\n        'trafficSource.adwordsClickInfo.page', 'sessionId', 'sessionStart', 'userId',\n        'locationZone', 'browserMajor', 'screenSize', 'gclIdPresent','trafficSource.adwordsClickInfo.'\n    ]\n    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n\n    if categorical_cols_to_encode is not None and len(categorical_cols_to_encode) > 0:\n        if fit_encoder:\n            from sklearn.preprocessing import OneHotEncoder\n            ohe_model = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n            transformed_data = ohe_model.fit_transform(df[categorical_cols_to_encode])\n        else:\n            transformed_data = ohe_model.transform(df[categorical_cols_to_encode])\n        \n        ohe_df = pd.DataFrame(transformed_data,\n                              columns=ohe_model.get_feature_names_out(categorical_cols_to_encode),\n                              index=df.index)\n        df = pd.concat([df.drop(columns=categorical_cols_to_encode), ohe_df], axis=1)\n\n    return df, ohe_model,tfidf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols = [\n    'userChannel','geoCluster','trafficSource','geoNetwork.subContinent','locationCountry','geoNetwork.continent','deviceType','os','browser','trafficSource.medium'\n]\n\nX_train_pre, ohe_model,tdf = preprocess(X_train, categorical_cols_to_encode=categorical_cols, fit_encoder=True)\nmedian_pageViews = X_train['pageViews'].median()\nX_test_pre, _,_ = preprocess(X_test, median_pageViews=median_pageViews,ohe_model=ohe_model, categorical_cols_to_encode=categorical_cols, fit_encoder=False,tfidf=tdf)\nX_predict_pre, _,_ = preprocess(X_predict, median_pageViews=median_pageViews,ohe_model=ohe_model, categorical_cols_to_encode=categorical_cols, fit_encoder=False,tfidf=tdf)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in X_train_pre.columns:\n    if 'userChannel' in i:\n        print(i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(list(X_predict_pre.columns))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\nX_train_pre.columns = [re.sub(r'[\\\"\\{\\}\\[\\]\\:\\n\\r\\t\\\\]', '_', col) for col in X_train_pre.columns]\nX_test_pre.columns = X_train_pre.columns\nX_predict_pre.columns = X_train_pre.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_pre = scaler.fit_transform(X_train_pre)\nX_test_pre = scaler.transform(X_test_pre)\nX_predict_pre = scaler.transform(X_predict_pre)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nlgbm = lgb.LGBMRegressor(\n    objective='regression_l1',\n    n_estimators=1000,\n    learning_rate=0.05,\n    num_leaves=31,          \n    max_depth=-1, \n    min_child_samples=20,  \n    subsample=0.8,     \n    colsample_bytree=0.8,     \n    reg_alpha=0.1,   \n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1\n)\nlgbm.fit(X_train_pre,Y_train)\nmodel = lgbm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=1000,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=0.1,\n    random_state=42\n)\nxgb.fit(X_train_pre, Y_train)\nmodel = xgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_pre, Y_train)\nmodel = ridge","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic.fit(X_train_pre, Y_train)\nmodel = elastic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\nrf.fit(X_train_pre, Y_train)\nmodel = rf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\net = ExtraTreesRegressor(n_estimators=100, max_depth=10, random_state=42)\net.fit(X_train_pre, Y_train)\nmodel = et","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(r2_score(Y_train,model.predict(X_train_pre)))\nprint(r2_score(Y_test,model.predict(X_test_pre)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y_predict = model.predict(X_predict_pre)\nY_predict = np.maximum(Y_predict, 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y = pd.DataFrame({'id': range(len(Y_predict)), 'purchaseValue': Y_predict})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y.to_csv(\"/kaggle/working/submission.csv\",index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}